{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63551e73-e57a-4e70-8085-96c15f68aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import emoji\n",
    "from datetime import datetime, date, timezone\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03ce5c-417d-4428-a6cd-0e042ee47d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "engine = create_engine(\n",
    "    f\"mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87efa149-bf9e-4465-af97-dd9478f160e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "\n",
    "    text = str(text).lower()\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r\"[^\\w\\s@.-]\", \"\", text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in STOP_WORDS]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619c732-aa11-48f5-b014-f7a834e7c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_email_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "\n",
    "    email_markers = [\"From:\", \"To:\", \"Subject:\", \"Message-ID:\", \"Date:\"]\n",
    "    return sum(marker in text for marker in email_markers) >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a71a6-6fbc-4b05-b103-38c4c90ee35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email_content(raw_text):\n",
    "    headers, _, body = raw_text.partition(\"\\n\\n\")\n",
    "\n",
    "    email_data = {\n",
    "        \"message_id\": None,\n",
    "        \"date\": None,\n",
    "        \"sender\": None,\n",
    "        \"receiver\": None,\n",
    "        \"subject\": None,\n",
    "        \"body\": body.strip()\n",
    "    }\n",
    "\n",
    "    for line in headers.split(\"\\n\"):\n",
    "        if line.startswith(\"Message-ID:\"):\n",
    "            email_data[\"message_id\"] = line.replace(\"Message-ID:\", \"\").strip()\n",
    "        elif line.startswith(\"Date:\"):\n",
    "            email_data[\"date\"] = line.replace(\"Date:\", \"\").strip()\n",
    "        elif line.startswith(\"From:\"):\n",
    "            email_data[\"sender\"] = line.replace(\"From:\", \"\").strip()\n",
    "        elif line.startswith(\"To:\"):\n",
    "            email_data[\"receiver\"] = line.replace(\"To:\", \"\").strip()\n",
    "        elif line.startswith(\"Subject:\"):\n",
    "            email_data[\"subject\"] = line.replace(\"Subject:\", \"\").strip()\n",
    "\n",
    "    return email_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fa968d-72b3-475a-9388-d08d4c3ae490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_email_data(email_dict):\n",
    "    for key in [\"subject\", \"body\"]:\n",
    "        if email_dict.get(key):\n",
    "            email_dict[key] = normalize_text(email_dict[key])\n",
    "    return email_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fdfae-29b0-4479-8eec-3e3d77ff1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing\n",
    "def normalize_all_tables_to_json(\n",
    "    engine,\n",
    "    output_file=\"outputs/normalized_output.json\",\n",
    "    chunk_size=1000\n",
    "):\n",
    "    tables = pd.read_sql(\"SHOW TABLES\", engine).iloc[:, 0].tolist()\n",
    "    table_id = 1\n",
    "    first_record = True  \n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\\n\")\n",
    "\n",
    "        for table in tables:\n",
    "            query = f\"SELECT * FROM `{table}`\"\n",
    "\n",
    "            # READ TABLE IN CHUNKS\n",
    "            for chunk in pd.read_sql(query, engine, chunksize=chunk_size):\n",
    "\n",
    "                for _, row in chunk.iterrows():\n",
    "                    cleaned_row = {}\n",
    "\n",
    "                    for column, value in row.items():\n",
    "\n",
    "                        # EMAIL DATA\n",
    "                        if isinstance(value, str) and detect_email_content(value):\n",
    "                            email_data = parse_email_content(value)\n",
    "                            email_data = normalize_email_data(email_data)\n",
    "                            cleaned_row[column] = email_data\n",
    "\n",
    "                        # TEXT DATA\n",
    "                        elif isinstance(value, str):\n",
    "                            cleaned_row[column] = normalize_text(value)\n",
    "\n",
    "                        # DATE / DATETIME\n",
    "                        elif isinstance(value, (datetime, date)):\n",
    "                            cleaned_row[column] = value.isoformat()\n",
    "\n",
    "                        # NON-TEXT\n",
    "                        else:\n",
    "                            cleaned_row[column] = value\n",
    "\n",
    "                    record = {\n",
    "                        \"id\": table_id,\n",
    "                        \"source_name\": table,\n",
    "                        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                        \"data\": cleaned_row\n",
    "                    }\n",
    "\n",
    "                    if not first_record:\n",
    "                        f.write(\",\\n\")\n",
    "                    else:\n",
    "                        first_record = False\n",
    "\n",
    "                    json.dump(record, f, ensure_ascii=False)\n",
    "\n",
    "            table_id += 1\n",
    "            print(table,\"done\")\n",
    "\n",
    "        f.write(\"\\n]\")\n",
    "\n",
    "    print(\"Normalization & JSON generation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2dae0ac-dafe-4259-afd3-287d8ca79543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer done\n",
      "customer_support_tickets done\n",
      "department done\n",
      "emails done\n",
      "employee done\n",
      "employee_project done\n",
      "glassdoor-companies-reviews done\n",
      "manager done\n",
      "order_items done\n",
      "orders done\n",
      "pdf done\n",
      "product done\n",
      "project done\n",
      "tata_motors_employee_reviews done\n",
      "Normalization & JSON generation completed\n"
     ]
    }
   ],
   "source": [
    "normalize_all_tables_to_json(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
