{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3fae03-2b18-46b2-8cf3-99dd611f39a4",
   "metadata": {},
   "source": [
    "## 1. Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021aa5b5-4755-4959-b4e2-ac59d63fd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"outputs/normalized_output.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40707c0f-86d2-4d49-9b01-9376da79735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities_from_value(value, field_name):\n",
    "    \"\"\"\n",
    "    Recursively extract entities from any JSON value\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "\n",
    "    if value is None:\n",
    "        return entities\n",
    "\n",
    "    # If nested dictionary\n",
    "    if isinstance(value, dict):\n",
    "        for k, v in value.items():\n",
    "            entities.extend(\n",
    "                extract_entities_from_value(v, f\"{field_name}.{k}\")\n",
    "            )\n",
    "        return entities\n",
    "\n",
    "    # If list\n",
    "    if isinstance(value, list):\n",
    "        for item in value:\n",
    "            entities.extend(\n",
    "                extract_entities_from_value(item, field_name)\n",
    "            )\n",
    "        return entities\n",
    "\n",
    "    # Convert value to text\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return entities\n",
    "\n",
    "    # Apply spaCy NER\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            \"entity\": ent.text,\n",
    "            \"label\": ent.label_,\n",
    "            \"source_field\": field_name\n",
    "        })\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def extract_entities_from_json_record(record):\n",
    "    extracted_entities = []\n",
    "\n",
    "    for field, value in record.items():\n",
    "        extracted_entities.extend(\n",
    "            extract_entities_from_value(value, field)\n",
    "        )\n",
    "\n",
    "    return extracted_entities\n",
    "\n",
    "\n",
    "def process_json_in_chunks(json_data, chunk_size=1000):\n",
    "    for i in range(0, len(json_data), chunk_size):\n",
    "        yield json_data[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48a63e-3a4a-43c7-ae73-a50e8ba09472",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "\n",
    "for chunk in process_json_in_chunks(json_data):\n",
    "    for record in chunk:\n",
    "        entities = extract_entities_from_json_record(record)\n",
    "\n",
    "        final_output.append({\n",
    "            \"id\": record.get(\"id\"),\n",
    "            \"source_name\": record.get(\"source_name\"),\n",
    "            \"timestamp\":record.get(\"timestamp\"),\n",
    "            \"entities\": entities\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf206336-1507-4300-a690-5dce08778a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"outputs/entity_extracted.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebab5e9",
   "metadata": {},
   "source": [
    "## 2,3. Relationship Extraction and Triple creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "INPUT_FILE = \"outputs/entity_extracted.jsonl\"\n",
    "OUTPUT_FILE = \"outputs/relationship_triples.jsonl\"\n",
    "\n",
    "def normalize(text):\n",
    "    return text.strip().title()\n",
    "\n",
    "def process_record_streaming(fin, fout):\n",
    "    record_count = 0\n",
    "    \n",
    "    for line in fin:\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        record_id = record.get(\"id\")\n",
    "        source_name = record.get(\"source_name\")\n",
    "        entities = record.get(\"entities\", [])\n",
    "        \n",
    "\n",
    "        # Use a set to store unique triples for this record to save space\n",
    "        unique_triples = set()\n",
    "        \n",
    "        norm_ents = []\n",
    "        for e in entities:\n",
    "            n_name = normalize(e[\"entity\"])\n",
    "            label = e.get(\"label\", \"\")\n",
    "            field = e.get(\"source_field\", \"\")\n",
    "            norm_ents.append((n_name, label))\n",
    "            \n",
    "            # Entity -> Source Field relationship\n",
    "            if field:\n",
    "                unique_triples.add((n_name, \"MENTIONED_IN\", field))\n",
    "        \n",
    "        # Pairwise relationships with deduplication\n",
    "        ent_len = len(norm_ents)\n",
    "        for i in range(ent_len):\n",
    "            subj, label1 = norm_ents[i]\n",
    "            for j in range(i + 1, ent_len):\n",
    "                obj, label2 = norm_ents[j]\n",
    "                \n",
    "                if subj == obj: continue\n",
    "                \n",
    "                pred = None\n",
    "                # Relationship Logic\n",
    "                if {label1, label2} <= {\"PERSON\", \"ORG\"}:\n",
    "                    pred = \"ASSOCIATED_WITH\"\n",
    "                elif label2 in (\"GPE\", \"LOC\") and label1 not in (\"GPE\", \"LOC\"):\n",
    "                    pred = \"LOCATED_IN\"\n",
    "                elif label1 in (\"GPE\", \"LOC\") and label2 not in (\"GPE\", \"LOC\"):\n",
    "                    # Flip order so it's always \"Thing LOCATED_IN Place\"\n",
    "                    unique_triples.add((obj, \"LOCATED_IN\", subj))\n",
    "                    continue\n",
    "\n",
    "                if pred:\n",
    "                    unique_triples.add((subj, pred, obj))\n",
    "\n",
    "        if unique_triples:\n",
    "            fout.write(json.dumps({\n",
    "                \"i\": record_id,\n",
    "                \"s\": source_name,\n",
    "                \"t\": list(unique_triples) \n",
    "            }, separators=(',', ':')) + \"\\n\")\n",
    "        \n",
    "        record_count += 1\n",
    "        if record_count % 10000 == 0:\n",
    "            print(f\"Processed {record_count} records...\")\n",
    "            gc.collect()\n",
    "            \n",
    "    return record_count\n",
    "\n",
    "gc.disable()\n",
    "try:\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "        total = process_record_streaming(fin, fout)\n",
    "        print(f\"Success! Total records: {total}\")\n",
    "finally:\n",
    "    gc.enable()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
